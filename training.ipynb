{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c6045a",
   "metadata": {},
   "source": [
    "Training file to detect pass or run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91be239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install torchvision\n",
    "# !pip install scikit-learn\n",
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "463cea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary libraries\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb66e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_frames(video_path):\n",
    "    \"\"\"\n",
    "    Reads a video and returns a list of raw RGB frames as NumPy arrays.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames_list = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert BGR (OpenCV) to RGB (NumPy/HuggingFace expectation)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames_list.append(rgb_frame)\n",
    "        \n",
    "    cap.release()\n",
    "    \n",
    "    # Hugging Face models often expect a list of frames, not a stacked tensor, at this stage\n",
    "    return frames_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494c3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom PyTorch Dataset class\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transforms):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.processor = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 1. Get raw frames (using the helper function)\n",
    "        raw_frames = extract_raw_frames(video_path)\n",
    "        \n",
    "        if not raw_frames:\n",
    "             # Handle a broken/empty video file (Crucial to prevent crashes!)\n",
    "             # In a real project, you'd likely filter these out pre-training.\n",
    "             return None, None \n",
    "\n",
    "        # 2. Use the TimesformerProcessor to transform the frames\n",
    "        # The 'padding' and 'return_tensors' arguments are essential.\n",
    "        encoding = self.processor(\n",
    "            images=raw_frames, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # The processor returns a dictionary. We extract the pixel values tensor.\n",
    "        # We also squeeze the tensor to remove the batch dimension (which is 1 here).\n",
    "        pixel_values = encoding['pixel_values'] \n",
    "\n",
    "        # 3. Return the processed tensor and the label\n",
    "        # The 'pixel_values' tensor is now in the required TimeSformer shape: \n",
    "        # (channels, num_frames, height, width)\n",
    "        return pixel_values, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c0b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_video_collate_fn(batch):\n",
    "    # 1. Filter out corrupted/empty videos\n",
    "    batch = [item for item in batch if item[0] is not None]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "\n",
    "    # 2. Separate and Prepare\n",
    "    pixel_values_list = [item[0].squeeze(0).clone() for item in batch]\n",
    "    labels = torch.LongTensor([item[1] for item in batch])\n",
    "    \n",
    "    # 3. Permute and Flatten (Preparation for Padding)\n",
    "    # Target shape for pad_sequence is (Frames, Features)\n",
    "    \n",
    "    # a. Permute: (C, F, H, W) -> (F, C, H, W)\n",
    "    permuted_tensors = [t.permute(1, 0, 2, 3) for t in pixel_values_list]\n",
    "\n",
    "    # b. Flatten: (F, C, H, W) -> (F, C*H*W)\n",
    "    # The new shape is (Frames, 3 * 224 * 224)\n",
    "    flattened_tensors = [t.flatten(1) for t in permuted_tensors]\n",
    "\n",
    "    # 4. Padding (Output shape: Batch, Max_F, C*H*W)\n",
    "    padded_sequences = torch.nn.utils.rnn.pad_sequence(\n",
    "        flattened_tensors, \n",
    "        batch_first=True, \n",
    "        padding_value=0.0\n",
    "    )\n",
    "\n",
    "    # --- 5. RESHAPE FIX ---\n",
    "    # Retrieve current dimensions after padding\n",
    "    batch_size, max_frames, _ = padded_sequences.shape \n",
    "    \n",
    "    # Unflatten the C*H*W back into (C, H, W)\n",
    "    # Shape: (Batch, Max_F, C, H, W)\n",
    "    final_inputs = padded_sequences.view(\n",
    "        batch_size, \n",
    "        max_frames, \n",
    "        3, 224, 224 # Fixed Dimensions: Channels, Height, Width\n",
    "    )\n",
    "\n",
    "    # 6. Final Permute for TimeSformer Input\n",
    "    # (B, F, C, H, W) -> (B, C, F, H, W)\n",
    "    final_inputs = final_inputs.permute(0, 2, 1, 3, 4) \n",
    "    \n",
    "    return final_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ac6cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jrsteel/.venv/lib64/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jrsteel/.venv/lib64/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_scratch/slurm.6730891/ipykernel_3735693/2886333734.py\", line 32, in custom_video_collate_fn\n    final_inputs = padded_sequences.view(\n                   ^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[2, 3, 3, 224, 224]' is invalid for input of size 108079104\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# 3. Reshape and Permute to Final 5D Tensor\u001b[39;00m\n\u001b[32m     81\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move data to the device\u001b[39;49;00m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:1516\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1514\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1515\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:1551\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jrsteel/.venv/lib64/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jrsteel/.venv/lib64/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_scratch/slurm.6730891/ipykernel_3735693/2886333734.py\", line 32, in custom_video_collate_fn\n    final_inputs = padded_sequences.view(\n                   ^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[2, 3, 3, 224, 224]' is invalid for input of size 108079104\n"
     ]
    }
   ],
   "source": [
    "## 1. Setup: Define Hyperparameters and Device\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 2\n",
    "frame_height = 224\n",
    "frame_width = 224\n",
    "num_channels = 3 # For RGB images\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the Dataset and DataLoader\n",
    "game = 'USCOffsensevClemsonPlays'\n",
    "# Get every file from the drive folder using os library\n",
    "import os\n",
    "video_paths = []\n",
    "file_names = []\n",
    "for root, dirs, files in os.walk(game):\n",
    "    for file in files:\n",
    "        if file.endswith('.mp4'):\n",
    "            video_paths.append(os.path.join(root, file))\n",
    "            file_names.append(file)\n",
    "# Split the filename by - and take the second part to get the label\n",
    "labels = [\n",
    "    int(f.split('-')[1][0]) for f in file_names if f.endswith('.mp4')\n",
    "]    # A list of corresponding labels (0 (run) or 1 (pass))\n",
    "\n",
    "\n",
    "from transformers import AutoModelForVideoClassification, AutoProcessor\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
    "\n",
    "dataset = VideoDataset(video_paths, labels, transforms=processor)\n",
    "\n",
    "total_size = len(dataset)\n",
    "\n",
    "# Split the data (e.g., 80% train, 20% test)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "from torch.utils.data import random_split\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader # provides TimeSformer (e.g., HuggingFace, or a custom im\n",
    "training_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=custom_video_collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 2. Load the Model and Adjust the Final Layer ---\n",
    "model_name = \"facebook/timesformer-base-finetuned-k400\"\n",
    "num_labels = 2 # Your two classes: 'run' and 'pass'\n",
    "\n",
    "# Use AutoModelForVideoClassification to load the TimeSformer architecture\n",
    "model = AutoModelForVideoClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels, # This correctly sets the output layer size to 2\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "for param in model.timesformer.parameters(): \n",
    "    param.requires_grad = False  # Freeze the main Transformer body\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set th\n",
    "\n",
    "    # 3. Reshape and Permute to Final 5D Tensor\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(training_dataloader):\n",
    "        # Move data to the device\n",
    "        inputs = inputs.permute(0, 2, 1, 3, 4).to(device)\n",
    "        labels = labels.to(device)\n",
    "        if inputs.dim() == 4:\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "        # The DataLoader may return None if a video is not loaded correctly\n",
    "        if inputs is None:\n",
    "            print(f\"Skipping batch {i} due to loading error.\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        del inputs, labels, outputs, loss  # Free up memory\n",
    "        torch.cuda.empty_cache()  # Clear the GPU cache if using GPU\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(training_dataloader):.4f}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state for testing use\n",
    "torch.save(model.state_dict(), 'video_classifier_model_attempt_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e5757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on test set...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 1. Forward Pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m loss = criterion(outputs, labels) \u001b[38;5;66;03m# Calculate loss for reporting\u001b[39;00m\n\u001b[32m     39\u001b[39m test_loss_total += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/transformers/models/timesformer/modeling_timesformer.py:723\u001b[39m, in \u001b[36mTimesformerForVideoClassification.forward\u001b[39m\u001b[34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    641\u001b[39m \u001b[33;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    719\u001b[39m \u001b[33;03meating spaghetti\u001b[39;00m\n\u001b[32m    720\u001b[39m \u001b[33;03m```\"\"\"\u001b[39;00m\n\u001b[32m    721\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimesformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m][:, \u001b[32m0\u001b[39m]\n\u001b[32m    732\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/transformers/models/timesformer/modeling_timesformer.py:589\u001b[39m, in \u001b[36mTimesformerModel.forward\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    584\u001b[39m output_hidden_states = (\n\u001b[32m    585\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    586\u001b[39m )\n\u001b[32m    587\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m    592\u001b[39m     embedding_output,\n\u001b[32m    593\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    594\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    595\u001b[39m     return_dict=return_dict,\n\u001b[32m    596\u001b[39m )\n\u001b[32m    597\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/transformers/models/timesformer/modeling_timesformer.py:98\u001b[39m, in \u001b[36mTimesformerEmbeddings.forward\u001b[39m\u001b[34m(self, pixel_values)\u001b[39m\n\u001b[32m     95\u001b[39m batch_size = pixel_values.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# create patch embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m embeddings, num_frames, patch_width = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m cls_tokens = \u001b[38;5;28mself\u001b[39m.cls_token.expand(embeddings.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    101\u001b[39m embeddings = torch.cat((cls_tokens, embeddings), dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib64/python3.11/site-packages/transformers/models/timesformer/modeling_timesformer.py:60\u001b[39m, in \u001b[36mTimesformerPatchEmbeddings.forward\u001b[39m\u001b[34m(self, pixel_values)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     batch_size, num_frames, num_channels, height, width = pixel_values.shape\n\u001b[32m     61\u001b[39m     pixel_values = pixel_values.reshape(batch_size * num_frames, num_channels, height, width)\n\u001b[32m     63\u001b[39m     embeddings = \u001b[38;5;28mself\u001b[39m.projection(pixel_values)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "# Create the test DataLoader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, # Use the same batch size or even larger for testing\n",
    "    shuffle=False,         # Crucial: DO NOT shuffle test data\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=video_collate_fn # Use the custom collate function\n",
    ")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "# This disables layers like Dropout and sets Batch Norm to use population statistics\n",
    "model.eval() \n",
    "\n",
    "# Lists to store true labels and predicted labels\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "test_loss_total = 0.0\n",
    "\n",
    "print(\"Starting evaluation on test set...\")\n",
    "\n",
    "# Turn off gradient calculations for memory and speed\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_dataloader):\n",
    "        # Move data to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if inputs is None:\n",
    "            continue\n",
    "            \n",
    "        # 1. Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels) # Calculate loss for reporting\n",
    "        test_loss_total += loss.item()\n",
    "\n",
    "        # 2. Get Predicted Class\n",
    "        # torch.max(outputs, 1) returns (values, indices)\n",
    "        # The indices are the predicted class (0 for run, 1 for pass)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 3. Store Results\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate the final average loss\n",
    "avg_test_loss = test_loss_total / len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13235b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance Metrics ---\n",
      "Overall Accuracy: 0.3333\n",
      "Precision (Class 1 'Pass'): 0.4286\n",
      "Recall (Class 1 'Pass'): 0.6000\n",
      "F1-Score (Class 1 'Pass'): 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Assuming you ran the evaluation loop above and have all_labels, all_predictions\n",
    "\n",
    "# Calculate Accuracy\n",
    "test_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Calculate Precision, Recall, F1-score\n",
    "# 'average=binary' is appropriate for a two-class problem where '1' is the positive class (Pass)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "    all_labels, \n",
    "    all_predictions, \n",
    "    average='binary', \n",
    "    pos_label=1  # Assuming '1' (Pass) is your positive class\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- Model Performance Metrics ---\")\n",
    "print(f\"Overall Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision (Class 1 'Pass'): {precision:.4f}\")\n",
    "print(f\"Recall (Class 1 'Pass'): {recall:.4f}\")\n",
    "print(f\"F1-Score (Class 1 'Pass'): {f1_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
