{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c6045a",
   "metadata": {},
   "source": [
    "Training file to detect pass or run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91be239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/jrsteel/.venv/lib64/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from triton==3.4.0->torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: numpy in /home/jrsteel/.venv/lib64/python3.11/site-packages (2.2.6)\n",
      "Requirement already satisfied: opencv-python in /home/jrsteel/.venv/lib64/python3.11/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from opencv-python) (2.2.6)\n",
      "Requirement already satisfied: torchvision in /home/jrsteel/.venv/lib64/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: torch==2.8.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torchvision) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from torch==2.8.0->torchvision) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from triton==3.4.0->torch==2.8.0->torchvision) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jrsteel/.venv/lib64/python3.11/site-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463cea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary libraries\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04420c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make video classifier \n",
    "\n",
    "class VideoClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VideoClassifier, self).__init__()\n",
    "        # Define layers\n",
    "        # Initialize the feature extractor (ResNet)\n",
    "        # We load a pre-trained ResNet-18 model\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        # Remove the final fully connected layer\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # The output of ResNet-50 is 2048-dimensional\n",
    "        resnet_feature_size = 512\n",
    "\n",
    "        # Create gru to process sequences of features\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size=resnet_feature_size, \n",
    "            hidden_size=512, \n",
    "            num_layers=1, \n",
    "            batch_first=True, \n",
    "        )\n",
    "\n",
    "        # Step 3: Initialize the final classification layer\n",
    "        # This takes the output of the GRU and makes a prediction\n",
    "        self.classifier = torch.nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The input 'x' is expected to be a tensor of shape:\n",
    "        # (batch_size, sequence_length, channels, height, width)\n",
    "        \n",
    "        # Get the dimensions for processing\n",
    "        batch_size, sequence_length, C, H, W = x.size()\n",
    "\n",
    "        # Reshape the input to process each frame through the ResNet\n",
    "        x_reshaped = x.view(batch_size * sequence_length, C, H, W)\n",
    "        \n",
    "        # Pass the reshaped tensor through the feature extractor (ResNet)\n",
    "        # This will get a feature vector for each frame\n",
    "        features = self.feature_extractor(x_reshaped)\n",
    "        \n",
    "        # The features will be of shape (batch_size * sequence_length, 2048, 1, 1)\n",
    "        # We need to flatten the last two dimensions to get a vector of size 2048\n",
    "        features = features.view(batch_size, sequence_length, -1)\n",
    "\n",
    "        # Pass the features through the GRU\n",
    "        # The GRU processes the sequence of features\n",
    "        gru_output, _ = self.gru(features)\n",
    "        \n",
    "        # We only need the output from the last timestep of the GRU\n",
    "        # This is the final state after processing the whole play\n",
    "        last_timestep_output = gru_output[:, -1, :]\n",
    "        \n",
    "        # Pass the last timestep output through the classifier to get the final prediction\n",
    "        prediction = self.classifier(last_timestep_output)\n",
    "        \n",
    "        return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de183b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the standard normalization values for models pre-trained on ImageNet\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# The complete preprocessing pipeline for a pre-trained ResNet\n",
    "preprocess = transforms.Compose([\n",
    "    # Convert the NumPy array to a PIL Image object\n",
    "    transforms.ToPILImage(),\n",
    "    # Resize the image to the standard input size for ResNet\n",
    "    transforms.Resize((224, 224)),\n",
    "    # Convert the image to a PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the tensor's pixel values\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "def load_video_as_tensor(video_path, preprocess_pipeline):\n",
    "    \"\"\"\n",
    "    Loads a video, preprocesses each frame, and returns a single tensor.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): The path to the video file.\n",
    "        preprocess_pipeline (transforms.Compose): The transformations to apply to each frame.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (sequence_length, channels, height, width).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames_list = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert the frame from BGR (OpenCV's default) to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply the preprocessing pipeline to the NumPy array\n",
    "        frame_tensor = preprocess_pipeline(rgb_frame)\n",
    "        frames_list.append(frame_tensor)\n",
    "        \n",
    "    cap.release()\n",
    "    \n",
    "    if frames_list:\n",
    "        # Stack all the frame tensors into a single tensor\n",
    "        return torch.stack(frames_list)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "# video_path = 'path/to/your/play.mp4'\n",
    "# video_tensor = load_video_as_tensor(video_path, preprocess)\n",
    "\n",
    "# if video_tensor is not None:\n",
    "#     print(f\"Loaded video tensor with shape: {video_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494c3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom PyTorch Dataset class\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transforms):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        video_tensor = load_video_as_tensor(video_path, self.transforms)\n",
    "        \n",
    "        return video_tensor, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ac6cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/10], Loss: 1.0139\n",
      "Epoch [2/10], Loss: 0.5345\n",
      "Epoch [3/10], Loss: 0.3236\n",
      "Epoch [4/10], Loss: 0.1410\n",
      "Epoch [5/10], Loss: 0.1858\n",
      "Epoch [6/10], Loss: 0.0404\n",
      "Epoch [7/10], Loss: 0.0816\n",
      "Epoch [8/10], Loss: 0.0114\n",
      "Epoch [9/10], Loss: 0.0230\n",
      "Epoch [10/10], Loss: 0.0078\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "## 1. Setup: Define Hyperparameters and Device\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 2\n",
    "frame_height = 224\n",
    "frame_width = 224\n",
    "num_channels = 3 # For RGB images\n",
    "\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the Dataset and DataLoader\n",
    "curr_drive = 'USCOffsensevClemsonPlays/plays_2nd_drive'\n",
    "# Get every file from the drive folder using os library\n",
    "\n",
    "import os\n",
    "\n",
    "video_paths = [\n",
    "    os.path.join(curr_drive, f) for f in os.listdir(curr_drive) if f.endswith('.mp4')\n",
    "] # A list of all your video clip file paths\n",
    "labels = [\n",
    "    0, 1, 0, 1\n",
    "]    # A list of corresponding labels (0 (run) or 1 (pass))\n",
    "\n",
    "dataset = VideoDataset(video_paths, labels, transforms=preprocess)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    "    )\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = VideoClassifier(num_classes=2).to(device)\n",
    "model.load_state_dict(torch.load('video_classifier_model_attempt_1.pth'))\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad = False  # Freeze the feature extractor\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        # Move data to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # The DataLoader may return None if a video is not loaded correctly\n",
    "        if inputs is None:\n",
    "            print(f\"Skipping batch {i} due to loading error.\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        del inputs, labels, outputs, loss  # Free up memory\n",
    "        torch.cuda.empty_cache()  # Clear the GPU cache if using GPU\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14b6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state for testing use\n",
    "\n",
    "torch.save(model.state_dict(), 'video_classifier_model_attempt_1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
